# =============================================================================
# MizanAI Chunking - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your credentials
# NEVER commit the .env file to version control!

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# Select which LLM provider to use for semantic chunking
# Options: gemini, openai, ollama, litellm
LLM_PROVIDER=gemini

# Model name for the selected provider
# Gemini: gemini-2.0-flash-lite, gemini-1.5-flash, gemini-1.5-pro
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Ollama: llama3.2, mistral, codellama
# LiteLLM: provider/model format (e.g., openai/gpt-4o-mini)
LLM_MODEL=gemini-2.0-flash-lite

# -----------------------------------------------------------------------------
# Google Gemini API Keys (for LLM and Embeddings)
# -----------------------------------------------------------------------------
# Get keys from: https://aistudio.google.com/app/apikey
# System automatically rotates between keys to avoid rate limits
# Add as many keys as you want (up to 10 supported)

GEMINI_API_KEY_1=your_gemini_api_key_1_here
GEMINI_API_KEY_2=your_gemini_api_key_2_here
GEMINI_API_KEY_3=your_gemini_api_key_3_here
GEMINI_API_KEY_4=your_gemini_api_key_4_here

# -----------------------------------------------------------------------------
# OpenAI API Configuration
# -----------------------------------------------------------------------------
# Get key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Custom base URL (for vLLM, OpenRouter, etc.)
# OPENAI_BASE_URL=https://api.openai.com/v1

# -----------------------------------------------------------------------------
# Ollama Configuration (Local LLM)
# -----------------------------------------------------------------------------
# Base URL for Ollama server (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# -----------------------------------------------------------------------------
# LiteLLM Configuration
# -----------------------------------------------------------------------------
# API key for LiteLLM (unified interface for 100+ LLMs)
LITELLM_API_KEY=your_litellm_api_key_here

# =============================================================================
# EMBEDDING PROVIDER CONFIGURATION
# =============================================================================
# Select which embedding provider to use
# Options: gemini, openai, ollama
EMBEDDING_PROVIDER=gemini

# Model name for embeddings
# Gemini: models/embedding-001 (768 dimensions)
# OpenAI: text-embedding-3-small (1536), text-embedding-3-large (3072)
# Ollama: nomic-embed-text, mxbai-embed-large
EMBEDDING_MODEL=models/embedding-001

# Embedding dimension (must match your model)
# Gemini embedding-001: 768
# OpenAI text-embedding-3-small: 1536
# OpenAI text-embedding-3-large: 3072
# Ollama nomic-embed-text: 768
EMBEDDING_DIMENSION=768

# =============================================================================
# VECTOR STORE CONFIGURATION
# =============================================================================
# Select which vector database to use
# Options: chromadb, supabase, pgvector, qdrant, weaviate, pinecone
VECTOR_STORE=chromadb

# Default collection/table/index name
COLLECTION_NAME=documents

# =============================================================================
# ChromaDB Configuration
# =============================================================================
# For ChromaDB Cloud (hosted)
# Get credentials from: https://www.trychroma.com/
CHROMADB_API_KEY=your_chromadb_api_key_here
CHROMADB_TENANT=your_chromadb_tenant_id_here
CHROMADB_DATABASE=your_chromadb_database_name_here

# For local ChromaDB (leave blank to use persistence)
# CHROMADB_HOST=localhost
# CHROMADB_PORT=8000

# =============================================================================
# Supabase Configuration (PostgreSQL + pgvector)
# =============================================================================
# Get credentials from: https://supabase.com/dashboard/project/_/settings/api
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_SERVICE_KEY=your_supabase_service_role_key_here
# SUPABASE_ANON_KEY=your_supabase_anon_key_here  # Optional

# =============================================================================
# PostgreSQL + pgvector Configuration
# =============================================================================
# Direct PostgreSQL connection with pgvector extension
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DATABASE=vectordb
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_postgres_password_here

# =============================================================================
# Qdrant Configuration
# =============================================================================
# For Qdrant Cloud
# Get credentials from: https://cloud.qdrant.io/
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key_here

# For local Qdrant
# QDRANT_URL=http://localhost:6333

# =============================================================================
# Weaviate Configuration
# =============================================================================
# For Weaviate Cloud
# Get credentials from: https://console.weaviate.cloud/
WEAVIATE_URL=https://your-cluster.weaviate.network
WEAVIATE_API_KEY=your_weaviate_api_key_here

# For local Weaviate
# WEAVIATE_URL=http://localhost:8080

# =============================================================================
# Pinecone Configuration
# =============================================================================
# Get credentials from: https://app.pinecone.io/
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=us-east-1-aws  # or your preferred region
PINECONE_INDEX=documents

# =============================================================================
# CHUNKING DEFAULTS
# =============================================================================
# Default chunk size in tokens
CHUNK_SIZE=512

# Default overlap between chunks in tokens
CHUNK_OVERLAP=50

# =============================================================================
# NOTES
# =============================================================================
# 1. Only configure the providers you plan to use
# 2. API keys are sensitive - NEVER commit .env to git
# 3. The system will auto-select based on your PROVIDER settings
# 4. You can override these settings via CLI arguments
# 5. For rate limit avoidance, add multiple Gemini keys (KEY_1, KEY_2, etc.)
